{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras-rectified-adam -q\n",
    "# !pip install transformers -q\n",
    "# !pip install tensorflow==2.1.0 -q\n",
    "# !pip install keras==2.2.5 -q\n",
    "# !pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "I0123 18:45:40.547617 139637616662336 file_utils.py:35] PyTorch version 1.3.1+cpu available.\n",
      "I0123 18:45:40.548414 139637616662336 file_utils.py:48] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import unidecode\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Activation, Input, Embedding, LSTM, Bidirectional, Dense, Dropout, SpatialDropout1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_radam import RAdam\n",
    "from random import randint, sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_cleaner(text):\n",
    "    try:\n",
    "        decoded = unidecode.unidecode(codecs.decode(text, 'unicode_escape'))\n",
    "    except:\n",
    "        decoded = unidecode.unidecode(text)\n",
    "    apostrophe_handled = re.sub(\"â€™\", \"'\", decoded)\n",
    "    expanded = ' '.join(\n",
    "        [contraction_mapping[t] if t in contraction_mapping else t for t in apostrophe_handled.split(\" \")])\n",
    "    parsed = nlp(expanded)\n",
    "    final_tokens = []\n",
    "    for t in parsed:\n",
    "        if t.is_punct or t.is_space or t.like_num or t.like_url or str(t).startswith('@'):\n",
    "            pass\n",
    "        else:\n",
    "            if t.lemma_ == '-PRON-':\n",
    "                final_tokens.append(str(t))\n",
    "            else:\n",
    "                sc_removed = re.sub(\"[^a-zA-Z]\", '', str(t.lemma_))\n",
    "                if len(sc_removed) > 1:\n",
    "                    final_tokens.append(sc_removed)\n",
    "    joined = ' '.join(final_tokens)\n",
    "    spell_corrected = re.sub(r'(.)\\1+', r'\\1\\1', joined)\n",
    "    return spell_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '2017_English_final/Subtask_A/'\n",
    "all_files = glob.glob(path + \"/twitter*.txt\")\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in all_files:\n",
    "    df = pd.read_csv(file_, index_col=None, sep='\\t', header=None, names=['id', 'sentiment', 'text', 'to_delete'])\n",
    "    list_.append(df.iloc[:, :-1])\n",
    "df = pd.concat(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "negative    7713\n",
      "positive      59\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamza/jupyter_virenv/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: invalid escape sequence '\\_'\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/hamza/jupyter_virenv/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: invalid escape sequence '\\/'\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/hamza/jupyter_virenv/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: invalid escape sequence '\\i'\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/hamza/jupyter_virenv/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: invalid escape sequence '\\ '\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df = df.drop(columns=['id'])\n",
    "df = df.reset_index(drop=True)\n",
    "df = df[df['sentiment'] != 'neutral']\n",
    "df = df[df['sentiment'] == 'negative'].append(df[df['sentiment'] == 'positive'].sample(frac =.003), ignore_index=True)\n",
    "\n",
    "df['token_length'] = [len(x.split(\" \")) for x in df.text]\n",
    "print(max(df.token_length))\n",
    "\n",
    "print(df.sentiment.value_counts())\n",
    "\n",
    "contraction_mapping = open('mapping.txt', 'r').read()\n",
    "contraction_mapping = eval(contraction_mapping)\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "df['clean_text'] = [spacy_cleaner(t) for t in df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17013</th>\n",
       "      <td>positive</td>\n",
       "      <td>I get to go see my endo tomorrow let just say ...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12508</th>\n",
       "      <td>positive</td>\n",
       "      <td>in Forbes rank Nestle as the th large public c...</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11188</th>\n",
       "      <td>positive</td>\n",
       "      <td>Batman may of be the well man in our last enco...</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15892</th>\n",
       "      <td>positive</td>\n",
       "      <td>Serena remain on top the young Williams bts Ve...</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11608</th>\n",
       "      <td>positive</td>\n",
       "      <td>do you enjoy last night Life of Chris Brown ne...</td>\n",
       "      <td>832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                         clean_text  word_count\n",
       "17013  positive  I get to go see my endo tomorrow let just say ...          43\n",
       "12508  positive  in Forbes rank Nestle as the th large public c...         110\n",
       "11188  positive  Batman may of be the well man in our last enco...         367\n",
       "15892  positive  Serena remain on top the young Williams bts Ve...         635\n",
       "11608  positive  do you enjoy last night Life of Chris Brown ne...         832"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word_count'] = df['clean_text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df[['sentiment','clean_text','word_count']].sort_values('word_count').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15262</th>\n",
       "      <td>positive</td>\n",
       "      <td>reply to this tweet with any challenge you may...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12508</th>\n",
       "      <td>positive</td>\n",
       "      <td>in Forbes rank Nestle as the th large public c...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11188</th>\n",
       "      <td>positive</td>\n",
       "      <td>Batman may of be the well man in our last enco...</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15892</th>\n",
       "      <td>positive</td>\n",
       "      <td>Serena remain on top the young Williams bts Ve...</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11608</th>\n",
       "      <td>positive</td>\n",
       "      <td>do you enjoy last night Life of Chris Brown ne...</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                         clean_text  stopwords\n",
       "15262  positive  reply to this tweet with any challenge you may...         22\n",
       "12508  positive  in Forbes rank Nestle as the th large public c...         29\n",
       "11188  positive  Batman may of be the well man in our last enco...        138\n",
       "15892  positive  Serena remain on top the young Williams bts Ve...        210\n",
       "11608  positive  do you enjoy last night Life of Chris Brown ne...        277"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df['stopwords'] = df['clean_text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "df[['sentiment','clean_text','stopwords']].sort_values('stopwords').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be', 'the', 'to', 'I', 'and', 'in', 'on', 'of', 'for', 'not']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words = pd.Series(' '.join(df['clean_text']).split()).value_counts()[:10].index.to_list()\n",
    "common_words\n",
    "# df['clean_text'] = df['clean_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>McGregor</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conor</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nd</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>night</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>not</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UFC</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutral</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>win</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fight</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>may</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>round</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>up</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>at</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>with</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>it</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>my</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>do</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>last</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>will</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>go</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>but</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mendes</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>talk</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>can</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Chad</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>have</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>see</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>interim</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>number</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>Neale</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>funny</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>hi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>cuz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>predict</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>your</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>link</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>fighter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>sit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>huge</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>consistently</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>find</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>demolish</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>wrong</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>look</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>wwe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>pillow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>Tim</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>AvecesSi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>gold</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>catch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>entertainer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>once</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>bet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>into</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>plus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>give</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>boy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>314 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            words  tf\n",
       "0        McGregor  38\n",
       "1           Conor  37\n",
       "2        positive  28\n",
       "3              nd  13\n",
       "4           night  11\n",
       "5              he  11\n",
       "6             not  11\n",
       "7             UFC  10\n",
       "8         neutral  10\n",
       "9             win   9\n",
       "10          fight   9\n",
       "11            may   9\n",
       "12          round   8\n",
       "13             up   7\n",
       "14             at   7\n",
       "15           with   7\n",
       "16             it   7\n",
       "17             my   7\n",
       "18             do   6\n",
       "19           last   6\n",
       "20           will   6\n",
       "21             go   6\n",
       "22            but   6\n",
       "23         Mendes   5\n",
       "24           talk   5\n",
       "25            can   5\n",
       "26           Chad   4\n",
       "27           have   4\n",
       "28            see   4\n",
       "29        interim   4\n",
       "..            ...  ..\n",
       "284        number   1\n",
       "285         Neale   1\n",
       "286         funny   1\n",
       "287            hi   1\n",
       "288           cuz   1\n",
       "289       predict   1\n",
       "290          your   1\n",
       "291          link   1\n",
       "292       fighter   1\n",
       "293           sit   1\n",
       "294          huge   1\n",
       "295  consistently   1\n",
       "296          find   1\n",
       "297      demolish   1\n",
       "298         wrong   1\n",
       "299          look   1\n",
       "300           wwe   1\n",
       "301        pillow   1\n",
       "302           Tim   1\n",
       "303      AvecesSi   1\n",
       "304          gold   1\n",
       "305         catch   1\n",
       "306        Friday   1\n",
       "307   entertainer   1\n",
       "308          once   1\n",
       "309           bet   1\n",
       "310          into   1\n",
       "311          plus   1\n",
       "312          give   1\n",
       "313           boy   1\n",
       "\n",
       "[314 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1 = (df['clean_text'][11608:11609]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>McGregor</td>\n",
       "      <td>38</td>\n",
       "      <td>6.274864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conor</td>\n",
       "      <td>37</td>\n",
       "      <td>6.248196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>28</td>\n",
       "      <td>6.389274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nd</td>\n",
       "      <td>13</td>\n",
       "      <td>1.065673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>night</td>\n",
       "      <td>11</td>\n",
       "      <td>2.231338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he</td>\n",
       "      <td>11</td>\n",
       "      <td>1.037560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>not</td>\n",
       "      <td>11</td>\n",
       "      <td>1.951985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UFC</td>\n",
       "      <td>10</td>\n",
       "      <td>6.196902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutral</td>\n",
       "      <td>10</td>\n",
       "      <td>8.276344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>win</td>\n",
       "      <td>9</td>\n",
       "      <td>3.073437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fight</td>\n",
       "      <td>9</td>\n",
       "      <td>4.749983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>may</td>\n",
       "      <td>9</td>\n",
       "      <td>2.109666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>round</td>\n",
       "      <td>8</td>\n",
       "      <td>4.215901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>up</td>\n",
       "      <td>7</td>\n",
       "      <td>2.326743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>at</td>\n",
       "      <td>7</td>\n",
       "      <td>0.647145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>with</td>\n",
       "      <td>7</td>\n",
       "      <td>1.781380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>it</td>\n",
       "      <td>7</td>\n",
       "      <td>0.710240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>my</td>\n",
       "      <td>7</td>\n",
       "      <td>2.071786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>do</td>\n",
       "      <td>6</td>\n",
       "      <td>1.829355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>last</td>\n",
       "      <td>6</td>\n",
       "      <td>3.436892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>will</td>\n",
       "      <td>6</td>\n",
       "      <td>2.246139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>go</td>\n",
       "      <td>6</td>\n",
       "      <td>1.340390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>but</td>\n",
       "      <td>6</td>\n",
       "      <td>2.525042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mendes</td>\n",
       "      <td>5</td>\n",
       "      <td>8.499487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>talk</td>\n",
       "      <td>5</td>\n",
       "      <td>4.577514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>can</td>\n",
       "      <td>5</td>\n",
       "      <td>2.416128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Chad</td>\n",
       "      <td>4</td>\n",
       "      <td>8.276344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>have</td>\n",
       "      <td>4</td>\n",
       "      <td>1.872770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>see</td>\n",
       "      <td>4</td>\n",
       "      <td>2.165764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>interim</td>\n",
       "      <td>4</td>\n",
       "      <td>9.885782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>number</td>\n",
       "      <td>1</td>\n",
       "      <td>5.993961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>Neale</td>\n",
       "      <td>1</td>\n",
       "      <td>9.885782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>funny</td>\n",
       "      <td>1</td>\n",
       "      <td>5.466941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>hi</td>\n",
       "      <td>1</td>\n",
       "      <td>1.234407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>cuz</td>\n",
       "      <td>1</td>\n",
       "      <td>6.451795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>predict</td>\n",
       "      <td>1</td>\n",
       "      <td>7.177732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>your</td>\n",
       "      <td>1</td>\n",
       "      <td>3.188747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>link</td>\n",
       "      <td>1</td>\n",
       "      <td>6.172210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>fighter</td>\n",
       "      <td>1</td>\n",
       "      <td>5.934538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>sit</td>\n",
       "      <td>1</td>\n",
       "      <td>3.675182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>huge</td>\n",
       "      <td>1</td>\n",
       "      <td>5.516334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>consistently</td>\n",
       "      <td>1</td>\n",
       "      <td>8.276344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>find</td>\n",
       "      <td>1</td>\n",
       "      <td>4.529195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>demolish</td>\n",
       "      <td>1</td>\n",
       "      <td>9.192635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>wrong</td>\n",
       "      <td>1</td>\n",
       "      <td>6.274864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>look</td>\n",
       "      <td>1</td>\n",
       "      <td>3.411891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>wwe</td>\n",
       "      <td>1</td>\n",
       "      <td>5.791437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>pillow</td>\n",
       "      <td>1</td>\n",
       "      <td>9.885782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>Tim</td>\n",
       "      <td>1</td>\n",
       "      <td>5.203650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>AvecesSi</td>\n",
       "      <td>1</td>\n",
       "      <td>9.885782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>gold</td>\n",
       "      <td>1</td>\n",
       "      <td>6.420046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>catch</td>\n",
       "      <td>1</td>\n",
       "      <td>5.222343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>2.757286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>entertainer</td>\n",
       "      <td>1</td>\n",
       "      <td>8.787169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>once</td>\n",
       "      <td>1</td>\n",
       "      <td>3.669176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>bet</td>\n",
       "      <td>1</td>\n",
       "      <td>4.250992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>into</td>\n",
       "      <td>1</td>\n",
       "      <td>4.460832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>plus</td>\n",
       "      <td>1</td>\n",
       "      <td>5.581717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>give</td>\n",
       "      <td>1</td>\n",
       "      <td>4.071651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>boy</td>\n",
       "      <td>1</td>\n",
       "      <td>4.396844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>314 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            words  tf       idf\n",
       "0        McGregor  38  6.274864\n",
       "1           Conor  37  6.248196\n",
       "2        positive  28  6.389274\n",
       "3              nd  13  1.065673\n",
       "4           night  11  2.231338\n",
       "5              he  11  1.037560\n",
       "6             not  11  1.951985\n",
       "7             UFC  10  6.196902\n",
       "8         neutral  10  8.276344\n",
       "9             win   9  3.073437\n",
       "10          fight   9  4.749983\n",
       "11            may   9  2.109666\n",
       "12          round   8  4.215901\n",
       "13             up   7  2.326743\n",
       "14             at   7  0.647145\n",
       "15           with   7  1.781380\n",
       "16             it   7  0.710240\n",
       "17             my   7  2.071786\n",
       "18             do   6  1.829355\n",
       "19           last   6  3.436892\n",
       "20           will   6  2.246139\n",
       "21             go   6  1.340390\n",
       "22            but   6  2.525042\n",
       "23         Mendes   5  8.499487\n",
       "24           talk   5  4.577514\n",
       "25            can   5  2.416128\n",
       "26           Chad   4  8.276344\n",
       "27           have   4  1.872770\n",
       "28            see   4  2.165764\n",
       "29        interim   4  9.885782\n",
       "..            ...  ..       ...\n",
       "284        number   1  5.993961\n",
       "285         Neale   1  9.885782\n",
       "286         funny   1  5.466941\n",
       "287            hi   1  1.234407\n",
       "288           cuz   1  6.451795\n",
       "289       predict   1  7.177732\n",
       "290          your   1  3.188747\n",
       "291          link   1  6.172210\n",
       "292       fighter   1  5.934538\n",
       "293           sit   1  3.675182\n",
       "294          huge   1  5.516334\n",
       "295  consistently   1  8.276344\n",
       "296          find   1  4.529195\n",
       "297      demolish   1  9.192635\n",
       "298         wrong   1  6.274864\n",
       "299          look   1  3.411891\n",
       "300           wwe   1  5.791437\n",
       "301        pillow   1  9.885782\n",
       "302           Tim   1  5.203650\n",
       "303      AvecesSi   1  9.885782\n",
       "304          gold   1  6.420046\n",
       "305         catch   1  5.222343\n",
       "306        Friday   1  2.757286\n",
       "307   entertainer   1  8.787169\n",
       "308          once   1  3.669176\n",
       "309           bet   1  4.250992\n",
       "310          into   1  4.460832\n",
       "311          plus   1  5.581717\n",
       "312          give   1  4.071651\n",
       "313           boy   1  4.396844\n",
       "\n",
       "[314 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,word in enumerate(tf1['words']):\n",
    "  tf1.loc[i, 'idf'] = np.log(df.shape[0]/(len(df[df['clean_text'].str.contains(word)])))\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>McGregor</td>\n",
       "      <td>38</td>\n",
       "      <td>6.274864</td>\n",
       "      <td>238.444825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conor</td>\n",
       "      <td>37</td>\n",
       "      <td>6.248196</td>\n",
       "      <td>231.183236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>28</td>\n",
       "      <td>6.389274</td>\n",
       "      <td>178.899677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nd</td>\n",
       "      <td>13</td>\n",
       "      <td>1.065673</td>\n",
       "      <td>13.853750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>night</td>\n",
       "      <td>11</td>\n",
       "      <td>2.231338</td>\n",
       "      <td>24.544723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he</td>\n",
       "      <td>11</td>\n",
       "      <td>1.037560</td>\n",
       "      <td>11.413156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>not</td>\n",
       "      <td>11</td>\n",
       "      <td>1.951985</td>\n",
       "      <td>21.471833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UFC</td>\n",
       "      <td>10</td>\n",
       "      <td>6.196902</td>\n",
       "      <td>61.969023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutral</td>\n",
       "      <td>10</td>\n",
       "      <td>8.276344</td>\n",
       "      <td>82.763438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>win</td>\n",
       "      <td>9</td>\n",
       "      <td>3.073437</td>\n",
       "      <td>27.660930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fight</td>\n",
       "      <td>9</td>\n",
       "      <td>4.749983</td>\n",
       "      <td>42.749850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>may</td>\n",
       "      <td>9</td>\n",
       "      <td>2.109666</td>\n",
       "      <td>18.986996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>round</td>\n",
       "      <td>8</td>\n",
       "      <td>4.215901</td>\n",
       "      <td>33.727206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>up</td>\n",
       "      <td>7</td>\n",
       "      <td>2.326743</td>\n",
       "      <td>16.287204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>at</td>\n",
       "      <td>7</td>\n",
       "      <td>0.647145</td>\n",
       "      <td>4.530018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>with</td>\n",
       "      <td>7</td>\n",
       "      <td>1.781380</td>\n",
       "      <td>12.469663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>it</td>\n",
       "      <td>7</td>\n",
       "      <td>0.710240</td>\n",
       "      <td>4.971679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>my</td>\n",
       "      <td>7</td>\n",
       "      <td>2.071786</td>\n",
       "      <td>14.502502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>do</td>\n",
       "      <td>6</td>\n",
       "      <td>1.829355</td>\n",
       "      <td>10.976130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>last</td>\n",
       "      <td>6</td>\n",
       "      <td>3.436892</td>\n",
       "      <td>20.621354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>will</td>\n",
       "      <td>6</td>\n",
       "      <td>2.246139</td>\n",
       "      <td>13.476837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>go</td>\n",
       "      <td>6</td>\n",
       "      <td>1.340390</td>\n",
       "      <td>8.042339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>but</td>\n",
       "      <td>6</td>\n",
       "      <td>2.525042</td>\n",
       "      <td>15.150251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mendes</td>\n",
       "      <td>5</td>\n",
       "      <td>8.499487</td>\n",
       "      <td>42.497437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>talk</td>\n",
       "      <td>5</td>\n",
       "      <td>4.577514</td>\n",
       "      <td>22.887570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>can</td>\n",
       "      <td>5</td>\n",
       "      <td>2.416128</td>\n",
       "      <td>12.080638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Chad</td>\n",
       "      <td>4</td>\n",
       "      <td>8.276344</td>\n",
       "      <td>33.105375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>have</td>\n",
       "      <td>4</td>\n",
       "      <td>1.872770</td>\n",
       "      <td>7.491078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>see</td>\n",
       "      <td>4</td>\n",
       "      <td>2.165764</td>\n",
       "      <td>8.663055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>interim</td>\n",
       "      <td>4</td>\n",
       "      <td>9.885782</td>\n",
       "      <td>39.543127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>number</td>\n",
       "      <td>1</td>\n",
       "      <td>5.993961</td>\n",
       "      <td>5.993961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>Neale</td>\n",
       "      <td>1</td>\n",
       "      <td>9.885782</td>\n",
       "      <td>9.885782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>funny</td>\n",
       "      <td>1</td>\n",
       "      <td>5.466941</td>\n",
       "      <td>5.466941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>hi</td>\n",
       "      <td>1</td>\n",
       "      <td>1.234407</td>\n",
       "      <td>1.234407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>cuz</td>\n",
       "      <td>1</td>\n",
       "      <td>6.451795</td>\n",
       "      <td>6.451795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>predict</td>\n",
       "      <td>1</td>\n",
       "      <td>7.177732</td>\n",
       "      <td>7.177732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>your</td>\n",
       "      <td>1</td>\n",
       "      <td>3.188747</td>\n",
       "      <td>3.188747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>link</td>\n",
       "      <td>1</td>\n",
       "      <td>6.172210</td>\n",
       "      <td>6.172210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>fighter</td>\n",
       "      <td>1</td>\n",
       "      <td>5.934538</td>\n",
       "      <td>5.934538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>sit</td>\n",
       "      <td>1</td>\n",
       "      <td>3.675182</td>\n",
       "      <td>3.675182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>huge</td>\n",
       "      <td>1</td>\n",
       "      <td>5.516334</td>\n",
       "      <td>5.516334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>consistently</td>\n",
       "      <td>1</td>\n",
       "      <td>8.276344</td>\n",
       "      <td>8.276344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>find</td>\n",
       "      <td>1</td>\n",
       "      <td>4.529195</td>\n",
       "      <td>4.529195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>demolish</td>\n",
       "      <td>1</td>\n",
       "      <td>9.192635</td>\n",
       "      <td>9.192635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>wrong</td>\n",
       "      <td>1</td>\n",
       "      <td>6.274864</td>\n",
       "      <td>6.274864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>look</td>\n",
       "      <td>1</td>\n",
       "      <td>3.411891</td>\n",
       "      <td>3.411891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>wwe</td>\n",
       "      <td>1</td>\n",
       "      <td>5.791437</td>\n",
       "      <td>5.791437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>pillow</td>\n",
       "      <td>1</td>\n",
       "      <td>9.885782</td>\n",
       "      <td>9.885782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>Tim</td>\n",
       "      <td>1</td>\n",
       "      <td>5.203650</td>\n",
       "      <td>5.203650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>AvecesSi</td>\n",
       "      <td>1</td>\n",
       "      <td>9.885782</td>\n",
       "      <td>9.885782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>gold</td>\n",
       "      <td>1</td>\n",
       "      <td>6.420046</td>\n",
       "      <td>6.420046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>catch</td>\n",
       "      <td>1</td>\n",
       "      <td>5.222343</td>\n",
       "      <td>5.222343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>2.757286</td>\n",
       "      <td>2.757286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>entertainer</td>\n",
       "      <td>1</td>\n",
       "      <td>8.787169</td>\n",
       "      <td>8.787169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>once</td>\n",
       "      <td>1</td>\n",
       "      <td>3.669176</td>\n",
       "      <td>3.669176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>bet</td>\n",
       "      <td>1</td>\n",
       "      <td>4.250992</td>\n",
       "      <td>4.250992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>into</td>\n",
       "      <td>1</td>\n",
       "      <td>4.460832</td>\n",
       "      <td>4.460832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>plus</td>\n",
       "      <td>1</td>\n",
       "      <td>5.581717</td>\n",
       "      <td>5.581717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>give</td>\n",
       "      <td>1</td>\n",
       "      <td>4.071651</td>\n",
       "      <td>4.071651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>boy</td>\n",
       "      <td>1</td>\n",
       "      <td>4.396844</td>\n",
       "      <td>4.396844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>314 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            words  tf       idf       tfidf\n",
       "0        McGregor  38  6.274864  238.444825\n",
       "1           Conor  37  6.248196  231.183236\n",
       "2        positive  28  6.389274  178.899677\n",
       "3              nd  13  1.065673   13.853750\n",
       "4           night  11  2.231338   24.544723\n",
       "5              he  11  1.037560   11.413156\n",
       "6             not  11  1.951985   21.471833\n",
       "7             UFC  10  6.196902   61.969023\n",
       "8         neutral  10  8.276344   82.763438\n",
       "9             win   9  3.073437   27.660930\n",
       "10          fight   9  4.749983   42.749850\n",
       "11            may   9  2.109666   18.986996\n",
       "12          round   8  4.215901   33.727206\n",
       "13             up   7  2.326743   16.287204\n",
       "14             at   7  0.647145    4.530018\n",
       "15           with   7  1.781380   12.469663\n",
       "16             it   7  0.710240    4.971679\n",
       "17             my   7  2.071786   14.502502\n",
       "18             do   6  1.829355   10.976130\n",
       "19           last   6  3.436892   20.621354\n",
       "20           will   6  2.246139   13.476837\n",
       "21             go   6  1.340390    8.042339\n",
       "22            but   6  2.525042   15.150251\n",
       "23         Mendes   5  8.499487   42.497437\n",
       "24           talk   5  4.577514   22.887570\n",
       "25            can   5  2.416128   12.080638\n",
       "26           Chad   4  8.276344   33.105375\n",
       "27           have   4  1.872770    7.491078\n",
       "28            see   4  2.165764    8.663055\n",
       "29        interim   4  9.885782   39.543127\n",
       "..            ...  ..       ...         ...\n",
       "284        number   1  5.993961    5.993961\n",
       "285         Neale   1  9.885782    9.885782\n",
       "286         funny   1  5.466941    5.466941\n",
       "287            hi   1  1.234407    1.234407\n",
       "288           cuz   1  6.451795    6.451795\n",
       "289       predict   1  7.177732    7.177732\n",
       "290          your   1  3.188747    3.188747\n",
       "291          link   1  6.172210    6.172210\n",
       "292       fighter   1  5.934538    5.934538\n",
       "293           sit   1  3.675182    3.675182\n",
       "294          huge   1  5.516334    5.516334\n",
       "295  consistently   1  8.276344    8.276344\n",
       "296          find   1  4.529195    4.529195\n",
       "297      demolish   1  9.192635    9.192635\n",
       "298         wrong   1  6.274864    6.274864\n",
       "299          look   1  3.411891    3.411891\n",
       "300           wwe   1  5.791437    5.791437\n",
       "301        pillow   1  9.885782    9.885782\n",
       "302           Tim   1  5.203650    5.203650\n",
       "303      AvecesSi   1  9.885782    9.885782\n",
       "304          gold   1  6.420046    6.420046\n",
       "305         catch   1  5.222343    5.222343\n",
       "306        Friday   1  2.757286    2.757286\n",
       "307   entertainer   1  8.787169    8.787169\n",
       "308          once   1  3.669176    3.669176\n",
       "309           bet   1  4.250992    4.250992\n",
       "310          into   1  4.460832    4.460832\n",
       "311          plus   1  5.581717    5.581717\n",
       "312          give   1  4.071651    4.071651\n",
       "313           boy   1  4.396844    4.396844\n",
       "\n",
       "[314 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dense = vectors.todense()\n",
    "# tf_df = pd.DataFrame(dense.tolist(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# roberta_tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "# roberta_sequence = TFRobertaForSequenceClassification.from_pretrained('distilroberta-base')\n",
    "# roberta_model = TFRobertaModel.from_pretrained('distilroberta-base')\n",
    "\n",
    "# x_data = [roberta_tokenizer.encode(val, max_length=30, pad_to_max_length=True) for val in list(df['clean_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = Input(shape=X[0].shape, dtype='int32')\n",
    "# alb, _ = roberta_model(inp)\n",
    "# lstm = LSTM(lstm_out, dropout=0.6, recurrent_dropout=0.6)(alb)\n",
    "# dense_out = Dense(2, activation='softmax')(lstm)\n",
    "# model = tf.keras.Model(inp, dense_out)\n",
    "# # model.layers[1].layers[1] = Dropout(0.5)\n",
    "# # model.layers[1].layers[2].activation = Activation('softmax')\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# model.load_weights('model.h5')\n",
    "# evaluatory_measures(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "lstm_out = 200\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=1)\n",
    "tokenizer.fit_on_texts(list(df['clean_text'].values))\n",
    "x_data = tokenizer.texts_to_sequences(df['clean_text'].values)\n",
    "x_data = pad_sequences(x_data, padding='post', maxlen=30, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n",
      "{'negative': 0, 'positive': 1}\n"
     ]
    }
   ],
   "source": [
    "y_data = df['sentiment'].values\n",
    "encoder = LabelEncoder()\n",
    "label = encoder.fit(y_data)\n",
    "labels_lookup = label.transform(y_data)\n",
    "print(labels_lookup)\n",
    "negative_label_lookup = label.inverse_transform(labels_lookup)\n",
    "negative_label_lookup = dict((x, y) for x, y in zip(negative_label_lookup, labels_lookup))\n",
    "print(negative_label_lookup)\n",
    "y_data = [negative_label_lookup[val] for val in y_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(x_data, y_data, stratify=y_data, test_size=0.2, random_state=36)\n",
    "\n",
    "def partition(data, n):\n",
    "    division = len(data) / float(n)\n",
    "    return np.array([data[int(round(division * i)): int(round(division * (i + 1)))] for i in range(n)])\n",
    "\n",
    "positive_val = negative_label_lookup['positive']\n",
    "negative_val = negative_label_lookup['negative']\n",
    "\n",
    "train_data = zip(X_train, Y_train)\n",
    "positive_data = [val for val in train_data if val[1] == positive_val]\n",
    "train_data = zip(X_train, Y_train)\n",
    "negative_data = [val for val in train_data if val[1] == negative_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positive_batch(p_data, n_data, batch_size, pos_ex_per_batch):\n",
    "    neg_ex_per_batch = int(len(n_data)/(batch_size - pos_ex_per_batch))\n",
    "    data = partition(n_data, neg_ex_per_batch)\n",
    "    # To create \"neg_ex_per_batch\" length of unique negative batches\n",
    "    pos_batch_length = len(p_data)//pos_ex_per_batch\n",
    "    # Since we have a loop running (n) times we need the outside loop to cater the index i.e.\n",
    "    # i.e. outside loop runs 6 times and inside runs 10 times total positive_batch will be 60 but the batch size of loop will be 6\n",
    "    batch = int(np.ceil(len(data)/pos_batch_length))\n",
    "    # Counter for positive indexes.\n",
    "    n_batch_i = 0\n",
    "    for index in range(batch):\n",
    "        # Array with indexes of length of negative data\n",
    "        example_indexes = list(np.arange(len(p_data)))\n",
    "        # Create batches of negative data and append each batch in each positive examples.\n",
    "        for i in range(pos_batch_length):\n",
    "            if n_batch_i >= len(data):\n",
    "                break\n",
    "\n",
    "            index_samp = sample(example_indexes, pos_ex_per_batch)\n",
    "            for p_i in index_samp:\n",
    "                data[n_batch_i].insert(randint(0, len(data[n_batch_i])), p_data[p_i])\n",
    "                # Remove used indexes from the list (avoid repetition)\n",
    "                example_indexes.remove(p_i)\n",
    "            n_batch_i += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_negative_batch(p_data, n_data, batch_size, neg_ex_per_batch):\n",
    "    pos_ex_per_batch = int(len(p_data)/(batch_size - neg_ex_per_batch))\n",
    "    data = partition(p_data, pos_ex_per_batch)\n",
    "    # To create \"neg_ex_per_batch\" length of unique negative batches\n",
    "    neg_batch_length = len(n_data)//neg_ex_per_batch\n",
    "    # Since we have a loop running (n) times we need the outside loop to cater the index i.e.\n",
    "    # i.e. outside loop runs 6 times and inside runs 10 times total positive_batch will be 60 but the batch size of loop will be 6\n",
    "    batch = int(np.ceil(len(data)/neg_batch_length))\n",
    "    # Counter for positive indexes.\n",
    "    p_batch_i = 0\n",
    "    for index in range(batch):\n",
    "        # Array with indexes of length of negative data\n",
    "        example_indexes = list(np.arange(len(n_data)))\n",
    "        # Create batches of negative data and append each batch in each positive examples.\n",
    "        for i in range(neg_batch_length):\n",
    "            if p_batch_i >= len(data):\n",
    "                break\n",
    "\n",
    "            index_samp = sample(example_indexes, neg_ex_per_batch)\n",
    "            for n_i in index_samp:\n",
    "                data[p_batch_i].insert(randint(0, len(data[p_batch_i])), n_data[n_i])\n",
    "                # Remove used indexes from the list (avoid repetition)\n",
    "                example_indexes.remove(n_i)\n",
    "            p_batch_i += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_examples_per_batch = 10\n",
    "pos_examples_per_batch = 10\n",
    "\n",
    "data = create_positive_batch(positive_data, negative_data, batch_size, pos_examples_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive values = 10\n",
      "Negative values = 54\n",
      "Total values = 64\n"
     ]
    }
   ],
   "source": [
    "print('Positive values = {}'.format(len([val for val in data[-1] if val[1] == positive_val])))\n",
    "print('Negative values = {}'.format(len([val for val in data[-1] if val[1] == negative_val])))\n",
    "print('Total values = {}'.format(len(data[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "\n",
    "for element in data:\n",
    "    for x_val, y_val in element:\n",
    "        X.append(x_val)\n",
    "        y.append(y_val)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "Y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0123 19:05:28.032369 139637616662336 word2vec.py:1588] collecting all words and their counts\n",
      "I0123 19:05:28.033116 139637616662336 word2vec.py:1573] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I0123 19:05:28.061026 139637616662336 word2vec.py:1596] collected 15212 word types from a corpus of 152665 raw words and 7772 sentences\n",
      "I0123 19:05:28.062661 139637616662336 word2vec.py:1647] Loading a fresh vocabulary\n",
      "I0123 19:05:28.089599 139637616662336 word2vec.py:1671] effective_min_count=1 retains 15212 unique words (100% of original 15212, drops 0)\n",
      "I0123 19:05:28.090431 139637616662336 word2vec.py:1677] effective_min_count=1 leaves 152665 word corpus (100% of original 152665, drops 0)\n",
      "I0123 19:05:28.165688 139637616662336 word2vec.py:1736] deleting the raw counts dictionary of 15212 items\n",
      "I0123 19:05:28.166849 139637616662336 word2vec.py:1739] sample=0.001 downsamples 50 most-common words\n",
      "I0123 19:05:28.167459 139637616662336 word2vec.py:1742] downsampling leaves estimated 116406 word corpus (76.2% of prior 152665)\n",
      "I0123 19:05:28.206385 139637616662336 base_any2vec.py:1022] estimated required memory for 15212 words and 300 dimensions: 44114800 bytes\n",
      "I0123 19:05:28.207154 139637616662336 word2vec.py:1888] resetting layer weights\n",
      "I0123 19:05:28.486121 139637616662336 base_any2vec.py:1210] training model with 3 workers on 15212 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=10 window=8\n",
      "I0123 19:05:28.719659 139637616662336 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I0123 19:05:28.721510 139637616662336 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I0123 19:05:28.727297 139637616662336 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I0123 19:05:28.728136 139637616662336 base_any2vec.py:1346] EPOCH - 1 : training on 152665 raw words (116466 effective words) took 0.2s, 495039 effective words/s\n",
      "I0123 19:05:28.955636 139637616662336 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I0123 19:05:28.960236 139637616662336 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I0123 19:05:28.962375 139637616662336 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I0123 19:05:28.963013 139637616662336 base_any2vec.py:1346] EPOCH - 2 : training on 152665 raw words (116449 effective words) took 0.2s, 508694 effective words/s\n",
      "I0123 19:05:29.188162 139637616662336 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I0123 19:05:29.188945 139637616662336 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I0123 19:05:29.196163 139637616662336 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I0123 19:05:29.197021 139637616662336 base_any2vec.py:1346] EPOCH - 3 : training on 152665 raw words (116300 effective words) took 0.2s, 506499 effective words/s\n",
      "I0123 19:05:29.412163 139637616662336 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I0123 19:05:29.417309 139637616662336 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I0123 19:05:29.420710 139637616662336 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I0123 19:05:29.421379 139637616662336 base_any2vec.py:1346] EPOCH - 4 : training on 152665 raw words (116436 effective words) took 0.2s, 530610 effective words/s\n",
      "I0123 19:05:29.651253 139637616662336 base_any2vec.py:349] worker thread finished; awaiting finish of 2 more threads\n",
      "I0123 19:05:29.655743 139637616662336 base_any2vec.py:349] worker thread finished; awaiting finish of 1 more threads\n",
      "I0123 19:05:29.662224 139637616662336 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I0123 19:05:29.662973 139637616662336 base_any2vec.py:1346] EPOCH - 5 : training on 152665 raw words (116249 effective words) took 0.2s, 492324 effective words/s\n",
      "I0123 19:05:29.663486 139637616662336 base_any2vec.py:1382] training on a 763325 raw words (581900 effective words) took 1.2s, 494570 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentences, sentiments = df['clean_text'].values, df['sentiment'].values\n",
    "tokenized_words = [word_tokenize(sent) for sent in sentences]\n",
    "model = Word2Vec(tokenized_words, size=300, window=8, min_count=1, negative=10)\n",
    "\n",
    "vocab_size = len(model.wv.index2word) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_sentiment = dict()\n",
    "# for words, sentiment in zip(tokenized_words, sentiments):\n",
    "#     for word in words:\n",
    "# #         if word not in word_sentiment.keys():\n",
    "#             word_sentiment.update({word: sentiment})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('outfile.tsv', 'w', encoding='utf8') as file_vector:\n",
    "#     with open('outfile.meta', 'w', encoding='utf8') as file_metadata:\n",
    "#         for word, sentiment in word_sentiment.items():\n",
    "#             if word in model.wv.index2word:\n",
    "#                 file_metadata.write(sentiment + '-' + word + '\\n')\n",
    "#                 vector_row = '\\t'.join(str(x) for x in model.wv[word])\n",
    "#                 file_vector.write(vector_row + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab_size):\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 300))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for i, word in enumerate(embedding.wv.index2word):\n",
    "        weight_matrix[i] = embedding.wv[word]\n",
    "    return weight_matrix\n",
    "\n",
    "embedding_vectors = get_weight_matrix(model, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def evaluatory_measures(model):\n",
    "    # model.evaluate(X_valid, Y_valid)\n",
    "    y_probs = model.predict(X_valid, verbose=0)\n",
    "    y_true = Y_valid\n",
    "#     y_pred = [int(round(x[0])) for x in y_probs]\n",
    "    \n",
    "#     y_true = [np.argmax(val, axis=None) for val in Y_valid]\n",
    "    y_pred = [np.argmax(val, axis=None) for val in y_probs]\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print('F1 score: %f' % f1)\n",
    "    # kappa\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    print('Cohens kappa: %f' % kappa)\n",
    "    # ROC AUC\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    print('ROC AUC: %f' % auc)\n",
    "    # confusion matrix\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(matrix)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    print('TN:{}, FP:{}, FN:{}, TP:{}'.format(tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0123 19:12:44.146194 139637616662336 data_adapter.py:1091] sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 30, 300)           4563900   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 200)               400800    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 4,965,102\n",
      "Trainable params: 401,202\n",
      "Non-trainable params: 4,563,900\n",
      "_________________________________________________________________\n",
      "Train on 7310 samples\n",
      "Epoch 1/3\n",
      "7310/7310 [==============================] - 50s 7ms/sample - loss: 0.1273 - accuracy: 0.4176\n",
      "Epoch 2/3\n",
      "7310/7310 [==============================] - 38s 5ms/sample - loss: 0.1244 - accuracy: 0.3973\n",
      "Epoch 3/3\n",
      "7310/7310 [==============================] - 42s 6ms/sample - loss: 0.1238 - accuracy: 0.3847\n",
      "Accuracy: 0.010289\n",
      "Precision: 0.007737\n",
      "Recall: 1.000000\n",
      "F1 score: 0.015355\n",
      "Cohens kappa: 0.000040\n",
      "ROC AUC: 0.501296\n",
      "[[   4 1539]\n",
      " [   0   12]]\n",
      "TN:4, FP:1539, FN:0, TP:12\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=X[0].shape))\n",
    "model.add(Embedding(vocab_size, embed_dim, weights=[embedding_vectors], trainable=False))\n",
    "model.add(LSTM(lstm_out, dropout=0.6, recurrent_dropout=0.6))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "class_weight = {0: 0.1,\n",
    "                1: 0.6}\n",
    "model.fit(X, Y, batch_size=batch_size, \n",
    "          epochs=3, class_weight=class_weight)\n",
    "evaluatory_measures(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_weight = {0: 30,\n",
    "                1: 0.05}\n",
    "model.fit(X, Y, batch_size=batch_size, epochs=5, class_weight=class_weight)\n",
    "evaluatory_measures(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0121 15:00:25.766878 140453695219520 data_adapter.py:1091] sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18547 samples\n",
      "Epoch 1/5\n",
      "18547/18547 [==============================] - 34s 2ms/sample - loss: 0.0176 - accuracy: 0.6945\n",
      "Epoch 2/5\n",
      "18547/18547 [==============================] - 34s 2ms/sample - loss: 0.0108 - accuracy: 0.7824\n",
      "Epoch 3/5\n",
      "18547/18547 [==============================] - 34s 2ms/sample - loss: 0.0094 - accuracy: 0.8057\n",
      "Epoch 4/5\n",
      "18547/18547 [==============================] - 43s 2ms/sample - loss: 0.0096 - accuracy: 0.8101\n",
      "Epoch 5/5\n",
      "18547/18547 [==============================] - 71s 4ms/sample - loss: 0.0103 - accuracy: 0.7955\n",
      "Accuracy: 0.790076\n",
      "Precision: 0.996146\n",
      "Recall: 0.792337\n",
      "F1 score: 0.882629\n",
      "Cohens kappa: -0.000278\n",
      "ROC AUC: 0.496169\n",
      "[[   3   12]\n",
      " [ 813 3102]]\n",
      "TN:3, FP:12, FN:813, TP:3102\n"
     ]
    }
   ],
   "source": [
    "class_weight = {0: 20,\n",
    "                1: 0.01}\n",
    "model.fit(X, Y, batch_size=batch_size, epochs=25, class_weight=class_weight)\n",
    "evaluatory_measures(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0121 11:17:04.128931 140453695219520 data_adapter.py:1091] sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18547 samples\n",
      "Epoch 1/15\n",
      "18547/18547 [==============================] - 40s 2ms/sample - loss: 0.1441 - accuracy: 0.6829\n",
      "Epoch 2/15\n",
      "18547/18547 [==============================] - 102s 6ms/sample - loss: 0.1298 - accuracy: 0.7002\n",
      "Epoch 3/15\n",
      "18547/18547 [==============================] - 99s 5ms/sample - loss: 0.1205 - accuracy: 0.7264\n",
      "Epoch 4/15\n",
      "18547/18547 [==============================] - 99s 5ms/sample - loss: 0.1178 - accuracy: 0.7288\n",
      "Epoch 5/15\n",
      "18547/18547 [==============================] - 93s 5ms/sample - loss: 0.1050 - accuracy: 0.7582\n",
      "Epoch 6/15\n",
      "18547/18547 [==============================] - 89s 5ms/sample - loss: 0.1109 - accuracy: 0.7590\n",
      "Epoch 7/15\n",
      "18547/18547 [==============================] - 88s 5ms/sample - loss: 0.0990 - accuracy: 0.7692\n",
      "Epoch 8/15\n",
      "18547/18547 [==============================] - 87s 5ms/sample - loss: 0.0856 - accuracy: 0.8030\n",
      "Epoch 9/15\n",
      "18547/18547 [==============================] - 77s 4ms/sample - loss: 0.0885 - accuracy: 0.8040\n",
      "Epoch 10/15\n",
      "18547/18547 [==============================] - 89s 5ms/sample - loss: 0.0817 - accuracy: 0.8106\n",
      "Epoch 11/15\n",
      "18547/18547 [==============================] - 89s 5ms/sample - loss: 0.0740 - accuracy: 0.8268\n",
      "Epoch 12/15\n",
      "18547/18547 [==============================] - 89s 5ms/sample - loss: 0.0768 - accuracy: 0.8315\n",
      "Epoch 13/15\n",
      "18547/18547 [==============================] - 92s 5ms/sample - loss: 0.0714 - accuracy: 0.8398\n",
      "Epoch 14/15\n",
      "18547/18547 [==============================] - 91s 5ms/sample - loss: 0.0691 - accuracy: 0.8405\n",
      "Epoch 15/15\n",
      "18547/18547 [==============================] - 87s 5ms/sample - loss: 0.0647 - accuracy: 0.8506\n",
      "Accuracy: 0.836896\n",
      "Precision: 0.996663\n",
      "Recall: 0.839080\n",
      "F1 score: 0.911108\n",
      "Cohens kappa: 0.004906\n",
      "ROC AUC: 0.552874\n",
      "[[   4   11]\n",
      " [ 630 3285]]\n",
      "TN:4, FP:11, FN:630, TP:3285\n"
     ]
    }
   ],
   "source": [
    "class_weight = {0: 20,\n",
    "                1: 0.1}\n",
    "model.fit(X, Y, batch_size=batch_size, epochs=15, class_weight=class_weight)\n",
    "evaluatory_measures(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0120 16:39:07.494138 140453695219520 data_adapter.py:1091] sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         8041800   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 400)               801600    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 802       \n",
      "=================================================================\n",
      "Total params: 8,844,202\n",
      "Trainable params: 802,402\n",
      "Non-trainable params: 8,041,800\n",
      "_________________________________________________________________\n",
      "Train on 18552 samples\n",
      "Epoch 1/5\n",
      "18552/18552 [==============================] - 102s 6ms/sample - loss: 1.1719 - accuracy: 0.5544\n",
      "Epoch 2/5\n",
      "18552/18552 [==============================] - 135s 7ms/sample - loss: 0.9257 - accuracy: 0.6837\n",
      "Epoch 3/5\n",
      "18552/18552 [==============================] - 263s 14ms/sample - loss: 0.6227 - accuracy: 0.8088\n",
      "Epoch 4/5\n",
      "18552/18552 [==============================] - 236s 13ms/sample - loss: 0.3834 - accuracy: 0.8919\n",
      "Epoch 5/5\n",
      "18552/18552 [==============================] - 234s 13ms/sample - loss: 0.2377 - accuracy: 0.9362\n",
      "Accuracy: 0.930025\n",
      "Precision: 0.995913\n",
      "Recall: 0.933589\n",
      "F1 score: 0.963744\n",
      "Cohens kappa: -0.007270\n",
      "ROC AUC: 0.466794\n",
      "[[   0   15]\n",
      " [ 260 3655]]\n",
      "TN:0, FP:15, FN:260, TP:3655\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim, weights=[embedding_vectors], trainable=False))\n",
    "model.add(LSTM(lstm_out, dropout=0.6, recurrent_dropout=0.6))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "class_weight = {0: 6.0,\n",
    "                1: 1.0}\n",
    "model.fit(X, Y, batch_size=batch_size, epochs=5, class_weight=class_weight)\n",
    "evaluatory_measures(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0120 15:00:42.146871 139930723264320 data_adapter.py:1091] sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, None, 300)         8041800   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 200)               400800    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 8,443,002\n",
      "Trainable params: 401,202\n",
      "Non-trainable params: 8,041,800\n",
      "_________________________________________________________________\n",
      "Train on 18547 samples\n",
      "Epoch 1/5\n",
      "18547/18547 [==============================] - 41s 2ms/sample - loss: 2.4146 - accuracy: 0.1571\n",
      "Epoch 2/5\n",
      "18547/18547 [==============================] - 63s 3ms/sample - loss: 2.3245 - accuracy: 0.1558\n",
      "Epoch 3/5\n",
      "18547/18547 [==============================] - 73s 4ms/sample - loss: 2.1912 - accuracy: 0.1740\n",
      "Epoch 4/5\n",
      "18547/18547 [==============================] - 73s 4ms/sample - loss: 2.1178 - accuracy: 0.2421\n",
      "Epoch 5/5\n",
      "18547/18547 [==============================] - 87s 5ms/sample - loss: 2.0013 - accuracy: 0.3326\n",
      "Accuracy: 0.136641\n",
      "Precision: 1.000000\n",
      "Recall: 0.133333\n",
      "F1 score: 0.235294\n",
      "Cohens kappa: 0.001173\n",
      "ROC AUC: 0.566667\n",
      "[[  15    0]\n",
      " [3393  522]]\n",
      "TN:15, FP:0, FN:3393, TP:522\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim, weights=[embedding_vectors], trainable=False))\n",
    "model.add(LSTM(lstm_out, dropout=0.6, recurrent_dropout=0.6))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "class_weight = {0: 30.0,\n",
    "                1: 1.0}\n",
    "model.fit(X, Y, batch_size=batch_size, epochs=5, class_weight=class_weight)\n",
    "evaluatory_measures(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0120 15:19:24.688891 139930723264320 data_adapter.py:1091] sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18547 samples\n",
      "Epoch 1/10\n",
      "18547/18547 [==============================] - 35s 2ms/sample - loss: 1.8301 - accuracy: 0.4106\n",
      "Epoch 2/10\n",
      "18547/18547 [==============================] - 35s 2ms/sample - loss: 1.6345 - accuracy: 0.5092\n",
      "Epoch 3/10\n",
      "18547/18547 [==============================] - 44s 2ms/sample - loss: 1.4332 - accuracy: 0.5840\n",
      "Epoch 4/10\n",
      "18547/18547 [==============================] - 74s 4ms/sample - loss: 1.1921 - accuracy: 0.6640\n",
      "Epoch 5/10\n",
      "18547/18547 [==============================] - 80s 4ms/sample - loss: 0.9527 - accuracy: 0.7391\n",
      "Epoch 6/10\n",
      "18547/18547 [==============================] - 91s 5ms/sample - loss: 0.8349 - accuracy: 0.7757\n",
      "Epoch 7/10\n",
      "18547/18547 [==============================] - 80s 4ms/sample - loss: 0.7183 - accuracy: 0.8165\n",
      "Epoch 8/10\n",
      "18547/18547 [==============================] - 78s 4ms/sample - loss: 0.5884 - accuracy: 0.8485\n",
      "Epoch 9/10\n",
      "18547/18547 [==============================] - 82s 4ms/sample - loss: 0.5346 - accuracy: 0.8647\n",
      "Epoch 10/10\n",
      "18547/18547 [==============================] - 81s 4ms/sample - loss: 0.4994 - accuracy: 0.8766\n",
      "Accuracy: 0.861578\n",
      "Precision: 0.997051\n",
      "Recall: 0.863602\n",
      "F1 score: 0.925541\n",
      "Cohens kappa: 0.010703\n",
      "ROC AUC: 0.598467\n",
      "[[   5   10]\n",
      " [ 534 3381]]\n",
      "TN:5, FP:10, FN:534, TP:3381\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, batch_size=batch_size, epochs=10, class_weight=class_weight)\n",
    "evaluatory_measures(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0120 13:14:28.164659 139930723264320 data_adapter.py:1091] sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 300)         8041800   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 200)               400800    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 8,443,002\n",
      "Trainable params: 401,202\n",
      "Non-trainable params: 8,041,800\n",
      "_________________________________________________________________\n",
      "Train on 18547 samples\n",
      "Epoch 1/5\n",
      "18547/18547 [==============================] - 75s 4ms/sample - loss: 1.2526 - accuracy: 0.4073\n",
      "Epoch 2/5\n",
      "18547/18547 [==============================] - 82s 4ms/sample - loss: 1.2222 - accuracy: 0.4584\n",
      "Epoch 3/5\n",
      "18547/18547 [==============================] - 76s 4ms/sample - loss: 1.1469 - accuracy: 0.5325\n",
      "Epoch 4/5\n",
      "18547/18547 [==============================] - 79s 4ms/sample - loss: 1.0855 - accuracy: 0.5906\n",
      "Epoch 5/5\n",
      "18547/18547 [==============================] - 81s 4ms/sample - loss: 1.0200 - accuracy: 0.6232\n",
      "Accuracy: 0.620102\n",
      "Precision: 0.997535\n",
      "Recall: 0.620179\n",
      "F1 score: 0.764845\n",
      "Cohens kappa: 0.004388\n",
      "ROC AUC: 0.610089\n",
      "[[   9    6]\n",
      " [1487 2428]]\n",
      "TN:9, FP:6, FN:1487, TP:2428\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim, weights=[embedding_vectors], trainable=False))\n",
    "model.add(LSTM(lstm_out, dropout=0.6, recurrent_dropout=0.6))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "class_weight = {0: 6.0,\n",
    "                1: 1.0}\n",
    "model.fit(X, Y, batch_size=batch_size, epochs=5, class_weight=class_weight)\n",
    "evaluatory_measures(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
